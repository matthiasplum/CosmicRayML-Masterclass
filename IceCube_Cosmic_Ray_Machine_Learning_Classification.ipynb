{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOygoix/AP0O6MpJorNNH7O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthiasplum/CosmicRayML-Masterclass/blob/main/IceCube_Cosmic_Ray_Machine_Learning_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to a Cosmic Ray Module:\n",
        "# Study Cosmic Rays with Machine Learning at the IceCube Neutrino Observatory\n",
        "\n",
        "Cosmic rays include particles such as electrons, protons, and heavier nuclei. Their sources are not yet known, but we expect them to be both galactic and extragalactic. The energy spectrum of cosmic rays, which is how the rate changes with increasing energy, has been measured by many detectors, including IceTop. Extragalactic sources are expected to be more abundant as energy increases and to be the only ones responsible for very high-energy cosmic rays.\n",
        "\n",
        "These cosmic messengers provide us with crucial insights into some of the most enigmatic phenomena in the universe, including the mysterious origins of cosmic rays themselves, the behavior of extreme astrophysical objects like black holes and supernovae, and the nature of dark matter and dark energy.\n",
        "\n",
        "The IceCube Neutrino Observatory is an extraordinary facility located at the South Pole. It is a cubic kilometer of ice embedded with thousands of light sensors, buried deep beneath the Antarctic surface. Using these sensors, IceCube detects neutrinos, elusive subatomic particles produced by cosmic ray interactions, providing astronomers with a unique window into the high-energy universe. IceCube is complemented by the surface detector array IceTop, which utilizes an array of 1 square kilometer and measures cosmic rays in the energy range from about 100 TeV to 1 EeV, which happens to be the region of the transition from galactic to extragalactic cosmic rays.\n",
        "\n",
        "In this module, we leverage the power of machine learning to delve deeper into the cosmic ray phenomena observed by IceCube and IceTop and showcase the methods to determine the mass composition of those particles.\n",
        "\n",
        " Machine learning, a branch of artificial intelligence, equips us with powerful tools to analyze complex datasets, recognize patterns, and make predictions. By applying machine learning techniques, we aim to uncover hidden insights, identify novel astrophysical sources, and enhance our understanding of cosmic ray physics.\n",
        "\n",
        "**Only synthetic data are used in this module, but the analysis techniques are the ones we are using all the time.**\n",
        "\n",
        "More information about the real analysis and the full scientific paper can be found here:\n",
        "\n",
        "[Cosmic Ray Spectrum and Composition from PeV to EeV Using 3 Years of Data From IceTop and IceCube](https://arxiv.org/abs/1906.04317)\n"
      ],
      "metadata": {
        "id": "WtSQV5eAHckX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://masterclass.icecube.wisc.edu/wp-content/uploads/aboutcr-2.png\" width=\"300\" style=\"background-color: white;\">\n",
        "</p>\n",
        "Energy spectrum of cosmic rays. The energy unit, the electronvolt (eV), is used by physicists to measure the energy and mass of relativistic particles. A typical chemical reaction happens in the eV energy regime, while nuclear reactions happen in the MeV (a million eV) range. The most powerful particle accelerator on Earth, the LHC, accelerates particles up to a few TeV (a million MeV). Cosmic rays reach energies above hundreds of EeV (a million TeV). Image: http://bit.ly/17gvx8C\n",
        "<p align=\"center\">\n",
        "<img src=\"https://masterclass.icecube.wisc.edu/wp-content/uploads/measuring-shower.png\" height=\"300\" />\n",
        "<img src=\"https://user-web.icecube.wisc.edu/~mplum/coincident.png\" height=\"300\" />\n",
        "</p>\n",
        "(left) A cosmic-ray shower reaches IceTop.\n",
        "(right) A 'coincident' cosmic ray event was measured in IceTop and IceCube simultaniously.\n"
      ],
      "metadata": {
        "id": "CZ_8QOdsTWAL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPVfA4EPcEjM"
      },
      "outputs": [],
      "source": [
        "# @title Import necessary libraries and create list to store training results <font color='Red' size=4>ONLY RUN THIS ONCE!!!!</font>\n",
        "# Import necessary libraries\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import powerlaw\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model ,to_categorical\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "#This is used to save your training tries... ONLY RUN THIS ONCE!!!!\n",
        "training_histories = []\n",
        "training_features_histories = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=4>Create some particle detector capability specifications for your experiment.</font>"
      ],
      "metadata": {
        "id": "dK98m6Up5Waw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify Detector capabilities and energy spectrum index\n",
        "energy_min = 10**6        #in GeV\n",
        "energy_max = 10**9        #in GeV\n",
        "zenith_min_deg = 0.       #in degree\n",
        "zenith_max_deg = 60.      #in degree\n",
        "spectral_index = -1.001     #Energy spectrum power law index"
      ],
      "metadata": {
        "id": "pA2k721YGe26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load helper functions\n",
        "#Define helper functions\n",
        "\n",
        "class TimePerEpochCallback(Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch_time = time.time() - self.epoch_start_time\n",
        "        self.times.append(epoch_time)\n",
        "        if logs is not None:\n",
        "            logs['time_per_epoch'] = epoch_time\n",
        "\n",
        "def plot_loghist(x, bins, ax):\n",
        "    hist, bins = np.histogram(x, bins=bins)\n",
        "    logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
        "    ax.hist(x, bins=logbins,alpha=0.5)\n",
        "    ax.set_xscale('log')\n",
        "\n",
        "def generate_power_law_data(num_events, energy_min, energy_max, spectral_index):\n",
        "    # Generate uniform random numbers\n",
        "    u = np.random.rand(num_events)\n",
        "\n",
        "    # Convert uniform random numbers to follow power-law distribution\n",
        "    energy = (energy_max**(spectral_index + 1) - energy_min**(spectral_index + 1)) * u + energy_min**(spectral_index + 1)\n",
        "    energy = energy**(1/(spectral_index + 1))\n",
        "    return energy\n",
        "\n",
        "def gaussian_random_xmax(energy, std_deviation=1.0, slope=0.05, intercept=0.):\n",
        "    # Calculate the mean as a linear function of energy\n",
        "    E0 = 1e9  # Reference energy in GeV\n",
        "    mean = intercept + slope*(np.log10(energy/E0)) # Example linear relation, adjust as needed\n",
        "    # Generate a Gaussian random number with the calculated mean and provided standard deviation\n",
        "    return np.random.normal(mean, std_deviation)\n",
        "\n",
        "def gaussian_random_with_increasing_mean(energy, std_deviation=1.0, slope=0.05, intercept=0.):\n",
        "    # Calculate the mean as a linear function of energy\n",
        "    mean = intercept + slope*(np.log10(energy)) # Example linear relation, adjust as needed\n",
        "    # Generate a Gaussian random number with the calculated mean and provided standard deviation\n",
        "    return np.random.normal(mean, std_deviation)\n",
        "\n",
        "def generate_cosmic_ray_labeled_data(n_events, particle_type):\n",
        "    #Generate Type label\n",
        "    ptype = [particle_type] * n_events\n",
        "\n",
        "    # Generate energies following a power-law distribution\n",
        "    energy = generate_power_law_data(n_events, energy_min, energy_max, spectral_index)\n",
        "\n",
        "    # Generate zenith angles uniformly in arccos between zenith_min_deg and zenith_max_deg\n",
        "    # Convert degrees to radians\n",
        "    zenith_min_rad = np.radians(zenith_min_deg)\n",
        "    zenith_max_rad = np.radians(zenith_max_deg)\n",
        "\n",
        "    # Generate uniformly distributed samples in the cosine of the angle\n",
        "    cos_theta_samples = np.random.uniform(np.cos(zenith_max_rad), np.cos(zenith_min_rad), n_events)\n",
        "\n",
        "    # Convert back to zenith angles\n",
        "    zenith_angles = np.arccos(cos_theta_samples)\n",
        "\n",
        "    # Generate air shower maximum\n",
        "    xmax = []\n",
        "    for i in range(n_events):\n",
        "      if particle_type =='proton':\n",
        "        xmax.append(gaussian_random_xmax(energy[i],slope=55,intercept=750,std_deviation=60))\n",
        "      elif particle_type =='helium':\n",
        "        xmax.append(gaussian_random_xmax(energy[i],slope=52,intercept=700,std_deviation=50))\n",
        "      elif particle_type =='oxygen':\n",
        "        xmax.append(gaussian_random_xmax(energy[i],slope=48,intercept=650,std_deviation=40))\n",
        "      elif particle_type =='iron':\n",
        "        xmax.append(gaussian_random_xmax(energy[i],slope=45,intercept=600,std_deviation=30))\n",
        "      else:\n",
        "        xmax.append(np.nan)\n",
        "    #xmax = np.random.normal(loc=800, scale=50, size=n_events)\n",
        "\n",
        "    # Generate high energy muon number proxy\n",
        "    muon_number = []\n",
        "    for i in range(n_events):\n",
        "      if particle_type =='proton':\n",
        "        muon_number.append(gaussian_random_with_increasing_mean(energy[i],std_deviation=0.2,slope=0.82,intercept=-3.58))\n",
        "      elif particle_type =='helium':\n",
        "        muon_number.append(gaussian_random_with_increasing_mean(energy[i],std_deviation=0.15,slope=0.81,intercept=-3.36))\n",
        "      elif particle_type =='oxygen':\n",
        "        muon_number.append(gaussian_random_with_increasing_mean(energy[i],std_deviation=0.10,slope=0.79,intercept=-3.14))\n",
        "      elif particle_type =='iron':\n",
        "        muon_number.append(gaussian_random_with_increasing_mean(energy[i],std_deviation=0.10,slope=0.78,intercept=-2.92))\n",
        "      else:\n",
        "        muon_number.append(np.nan)\n",
        "\n",
        "    # Create panda DataFrame\n",
        "    cosmic_ray_data = pd.DataFrame({\n",
        "        'Type': ptype,\n",
        "        'Energy': energy,\n",
        "        'Log10Energy': np.log10(energy),\n",
        "        'Zenith': zenith_angles,\n",
        "        'MuonNumberProxy': muon_number,\n",
        "        'Xmax': xmax\n",
        "    })\n",
        "\n",
        "    return cosmic_ray_data\n",
        "\n",
        "event_type = {\n",
        "    \"all_raw\": [\"Log10Energy\", \"Zenith\", \"Xmax\", \"MuonNumberProxy\"],\n",
        "    \"energy+zenith\": [\"Log10Energy\", \"Zenith\"],\n",
        "    \"energy+zenith+xmax\": [\"Log10Energy\", \"Zenith\", \"Xmax\"],\n",
        "    \"energy+zenith+muon\": [\"Log10Energy\", \"Zenith\", \"MuonNumberProxy\"],\n",
        "    \"all+log\": [\"Log10Energy\", \"Zenith\", \"Xmax\", \"MuonNumberProxy\", \"Log_Xmax\"],\n",
        "    \"all+ratio\": [\"Log10Energy\", \"Zenith\", \"Xmax\", \"MuonNumberProxy\",\\\n",
        "                  \"Energy_Muon_Ratio\", \"Energy_Xmax_Ratio\", \"Muon_Xmax_Ratio\"],\n",
        "    \"all+product\": [\"Log10Energy\", \"Zenith\", \"Xmax\", \"MuonNumberProxy\",\\\n",
        "                    \"Energy_Muon_Product\", \"Energy_Xmax_Product\", \"Muon_Xmax_Product\"],\n",
        "    \"all\": [\"Log10Energy\", \"Zenith\", \"Xmax\", \"MuonNumberProxy\", \\\n",
        "            \"Log_Xmax\",\\\n",
        "            \"Energy_Muon_Ratio\", \"Energy_Xmax_Ratio\", \"Muon_Xmax_Ratio\",\\\n",
        "            \"Energy_Muon_Product\", \"Energy_Xmax_Product\", \"Muon_Xmax_Product\"]\n",
        "}"
      ],
      "metadata": {
        "id": "tJCybErs0J-0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Orange' size=5>You can change this value if you want more synthetic data!</font>\n"
      ],
      "metadata": {
        "id": "dqINPfL-ywBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the size of your data sets for proton and iron\n",
        "num_events = 10000"
      ],
      "metadata": {
        "id": "SPP9NoGmLsqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the syntetic data sets\n",
        "#Create labeled data sets for proton and iron cosmic rays\n",
        "df_proton = generate_cosmic_ray_labeled_data(num_events,'proton')\n",
        "df_helium = generate_cosmic_ray_labeled_data(num_events,'helium')\n",
        "df_oxygen = generate_cosmic_ray_labeled_data(num_events,'oxygen')\n",
        "df_iron = generate_cosmic_ray_labeled_data(num_events,'iron')\n",
        "\n",
        "#Combine labeled datasets to your analysis data set\n",
        "data = pd.concat([df_proton, df_helium, df_oxygen, df_iron], ignore_index=True)"
      ],
      "metadata": {
        "id": "nM7JZFWMBvNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the dataset\n",
        "print(data.head())  # Check the first few rows\n",
        "print(\" \")\n",
        "print(data.info())  # Get information about columns and data types\n",
        "#print(data.describe())  # Statistical summary of numerical columns"
      ],
      "metadata": {
        "id": "6fmxBcB_jYzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot the synthetic data set\n",
        "# Plotting the measured distributions\n",
        "fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "plot_loghist(df_proton['Energy'],20,ax1)\n",
        "plot_loghist(df_helium['Energy'],20,ax1)\n",
        "plot_loghist(df_oxygen['Energy'],20,ax1)\n",
        "plot_loghist(df_iron['Energy'],20,ax1)\n",
        "ax1.legend(['Proton','Helium','Oxygen','Iron'])\n",
        "ax1.set_yscale('log')\n",
        "ax1.set_xlabel('Energy (GeV)')\n",
        "ax1.set_ylabel('Number of events')\n",
        "ax1.set_title('Synthetic Cosmic Ray Energy Distribution')\n",
        "\n",
        "ax2.hist(df_proton['Zenith'],20,alpha=0.5,label='Proton')\n",
        "ax2.hist(df_helium['Zenith'],20,alpha=0.5,label='Helium')\n",
        "ax2.hist(df_oxygen['Zenith'],20,alpha=0.5,label='Oxygen')\n",
        "ax2.hist(df_iron['Zenith'],20,alpha=0.5,label='Iron')\n",
        "\n",
        "ax2.legend()\n",
        "ax2.set_xlabel('Zenith in radians')\n",
        "ax2.set_ylabel('Number of events')\n",
        "ax2.set_title('Synthetic Cosmic Ray Zenith Distribution')\n",
        "\n",
        "ax3.scatter(df_proton['Energy'],df_proton['Xmax'],alpha=0.25,marker='.',label='Proton')\n",
        "ax3.scatter(df_helium['Energy'],df_helium['Xmax'],alpha=0.25,marker='.',label='Helium')\n",
        "ax3.scatter(df_oxygen['Energy'],df_oxygen['Xmax'],alpha=0.25,marker='.',label='Oxygen')\n",
        "ax3.scatter(df_iron['Energy'],df_iron['Xmax'],alpha=0.25,marker='.',label='Iron')\n",
        "'''\n",
        "sns.regplot(x=df_proton['Energy'], y=df_proton['Xmax'], ax=ax3, scatter=False, logx=True, label='Proton')\n",
        "sns.regplot(x=df_helium['Energy'], y=df_helium['Xmax'], ax=ax3, scatter=False, logx=True, label='Helium')\n",
        "sns.regplot(x=df_oxygen['Energy'], y=df_oxygen['Xmax'], ax=ax3, scatter=False, logx=True, label='Oxygen')\n",
        "sns.regplot(x=df_iron['Energy'], y=df_iron['Xmax'], ax=ax3, scatter=False, logx=True, label='Iron')\n",
        "'''\n",
        "\n",
        "ax3.legend()\n",
        "ax3.set_title('Synthetic Cosmic Ray Xmax Distribution')\n",
        "\n",
        "ax3.set_xscale('log')\n",
        "ax3.set_xlabel('Energy (GeV)')\n",
        "ax3.set_ylabel(r'Air shower maximum in g/cm$^2$')\n",
        "\n",
        "ax4.scatter(df_proton['Energy'],df_proton['MuonNumberProxy'],alpha=0.25,marker='.',label='Proton')\n",
        "ax4.scatter(df_helium['Energy'],df_helium['MuonNumberProxy'],alpha=0.25,marker='.',label='Helium')\n",
        "ax4.scatter(df_oxygen['Energy'],df_oxygen['MuonNumberProxy'],alpha=0.25,marker='.',label='Oxygen')\n",
        "ax4.scatter(df_iron['Energy'],df_iron['MuonNumberProxy'],alpha=0.25,marker='.',label='Iron')\n",
        "\n",
        "ax4.legend()\n",
        "ax4.set_title('Synthetic Cosmic Ray HE Muon Distribution')\n",
        "ax4.set_xscale('log')\n",
        "ax4.set_xlabel('Energy (GeV)')\n",
        "ax4.set_ylabel('High-energy muon number proxy')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9hGQqfxUtMIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "Pf6gjPSm8df_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since cosmic ray data often spans several orders of magnitude, log transformations can be useful."
      ],
      "metadata": {
        "id": "yA7dlmD39yAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Log_Xmax'] = np.log(data['Xmax'])"
      ],
      "metadata": {
        "id": "Kryd94J09xLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, [ax1,ax2] = plt.subplots(1, 2, figsize=(10, 5))\n",
        "sns.scatterplot(data=data, x='Energy', y='Log_Xmax', hue='Type',s=5 , ax=ax1)\n",
        "ax1.set_title('Energy vs Ln(Xmax)')\n",
        "\n",
        "\n",
        "ax2.set_xlabel('ln(Xmax)')\n",
        "ax2.set_ylabel('Number of events')\n",
        "ax2.set_title('Synthetic Cosmic Ray Ln(Xmax) Distribution')\n",
        "sns.histplot(data=data, x='Log_Xmax', kde=False, ax=ax2,hue='Type')"
      ],
      "metadata": {
        "id": "_6Rvbxx5-Jl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating ratios or interaction terms between features can sometimes capture important relationships. These are just some possible combinations.\n"
      ],
      "metadata": {
        "id": "-nYuEENc9bfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Energy_Muon_Ratio']   = data['Log10Energy'] / (data['MuonNumberProxy'] + 1)\n",
        "data['Energy_Xmax_Ratio']   = data['Log10Energy'] / (data['Xmax'] + 1)\n",
        "data['Muon_Xmax_Ratio']     = data['MuonNumberProxy'] / (data['Xmax'] + 1)\n",
        "data['Energy_Muon_Product'] = data['Log10Energy'] * data['MuonNumberProxy']\n",
        "data['Energy_Xmax_Product'] = data['Log10Energy'] * data['Xmax']\n",
        "data['Muon_Xmax_Product']   = data['MuonNumberProxy'] * data['Xmax']"
      ],
      "metadata": {
        "id": "odyL59o58ai6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ([ax1, ax2, ax3], [ax4, ax5, ax6]) = plt.subplots(2, 3, figsize=(15, 10))\n",
        "sns.scatterplot(data=data, x='Log10Energy', y='Energy_Muon_Ratio', hue='Type',s=5 , ax=ax1)\n",
        "ax1.set_title('Energy_Muon_Ratio vs Log(Energy)')\n",
        "sns.scatterplot(data=data, x='Log10Energy', y='Energy_Xmax_Ratio', hue='Type',s=5 , ax=ax2)\n",
        "ax2.set_title('Energy_Xmax_Ratio vs Log(Energy)')\n",
        "sns.scatterplot(data=data, x='Log10Energy', y='Muon_Xmax_Ratio', hue='Type',s=5 , ax=ax3)\n",
        "ax3.set_title('Muon_Xmax_Ratio vs MuonNumberProxy')\n",
        "sns.scatterplot(data=data, x='Xmax', y='Energy_Xmax_Ratio', hue='Type',s=5 , ax=ax4)\n",
        "ax4.set_title('Energy_Xmax_Ratio vs Xmax')\n",
        "sns.scatterplot(data=data, x='Log10Energy', y='Energy_Xmax_Product', hue='Type',s=5 , ax=ax5)\n",
        "ax5.set_title('Energy_Muon_Product vs Log(Energy)')\n",
        "sns.scatterplot(data=data, x='Log10Energy', y='Muon_Xmax_Product', hue='Type',s=5 , ax=ax6)\n",
        "ax6.set_title('Energy_Muon_Product vs Log(Energy)')\n",
        "\n"
      ],
      "metadata": {
        "id": "PtiLRcae85nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Orange' size=5>**Are all observables carrying information about the primary mass?**</font>"
      ],
      "metadata": {
        "id": "M1sAn5XcXMwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Orange' size=5>**Please select the features you want to train on!**"
      ],
      "metadata": {
        "id": "h5KnQJkgQm8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainevents = event_type['energy+zenith']\n",
        "\n",
        "### Raw features\n",
        "#event_type['energy+zenith']\n",
        "#event_type['energy+zenith+xmax']\n",
        "#event_type['energy+zenith+muon']\n",
        "#event_type['all_raw']\n",
        "\n",
        "### Add feature engineering\n",
        "### Raw features + Logarithmized Xmax\n",
        "#event_type['all+log']\n",
        "\n",
        "### Raw features + Ratio\n",
        "#event_type['all+ratio']\n",
        "\n",
        "### Raw features + Product\n",
        "#event_type['all+product']\n",
        "\n",
        "### Raw features + Logarithmized Xmax + Ratios + Products\n",
        "#event_type['all']"
      ],
      "metadata": {
        "id": "ScshCAxDJZUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing of data\n",
        "Preprocessing of data involves getting your data ready for machine learning algorithms by cleaning, transforming, and organizing it in a format that the model can understand and learn from effectively.\n",
        "\n",
        "Encoding Categorical Variables:\n",
        "\n",
        "* If your dataset contains categorical variables (like colors, categories, etc.), you need to convert them into a numerical format because most machine learning algorithms only understand numerical data. One common method is Label Encoding, which assigns a unique number to each category.\n",
        "\n",
        "Splitting the Data:\n",
        "* Divide your dataset into two subsets: a training set and a testing set. The training set is used to train the model, while the testing set is used to evaluate its performance. This helps to assess how well the model will generalize to new, unseen data.\n",
        "\n",
        "Feature Scaling:\n",
        "* It's important to scale your features to a similar range to prevent some features from dominating others. Standardization and normalization are two common techniques used for feature scaling. Standardization scales the data to have a mean of 0 and a standard deviation of 1, while normalization scales the data to a range between 0 and 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "77csVQYFvK0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing of the data\n",
        "\n",
        "# Encode categorical variables if any. In this case transform the name of the particle into a number.\n",
        "label_encoder = LabelEncoder()\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == 'object':\n",
        "        data['Encoded_'+column] = label_encoder.fit_transform(data[column])\n",
        "\n",
        "#Drop unused columns\n",
        "#data = data.drop(columns=['Energy','Type'])\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = data.drop(['Encoded_Type'], axis=1)  # Features\n",
        "\n",
        "y = data['Encoded_Type']                 # Target variable\n",
        "y_categorical = to_categorical(y)        # Split in categorical shape\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[trainevents], y_categorical, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "uJaV5gTQfNt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning neural network parameter\n",
        "\n",
        "<font color='Orange' size=4>These are some of the neural network parameters that you can tune and test how they effect your training.</font>\n",
        "\n",
        "Loss Function:\n",
        "\n",
        "* It's a measure of how well the model is performing during training.\n",
        "* Think of it as a way to calculate how \"wrong\" the model's predictions are compared to the actual values.\n",
        "* The goal during training is to minimize this \"wrongness\" or loss.\n",
        "* Common loss functions include Mean Squared Error (MSE) for regression tasks and Binary Crossentropy or Categorical Crossentropy for classification tasks.\n",
        "\n",
        "Batch Size:\n",
        "\n",
        "* It's the number of samples used in one iteration of training.\n",
        "* Imagine your dataset is a big book. Instead of reading the entire book at once, you read it page by page. Each page represents a batch.\n",
        "* Using smaller batch sizes helps in faster training and better utilization of memory resources, especially for large datasets.\n",
        "\n",
        "Number of Epochs:\n",
        "\n",
        "* An epoch is one complete pass through the entire training dataset.\n",
        "* The number of epochs defines how many times the model will see the entire dataset during training.\n",
        "* Just like reading the same book multiple times to remember it better, training for multiple epochs helps the model learn patterns in the data more effectively.\n",
        "* However, training for too many epochs can lead to overfitting, where the model memorizes the training data instead of learning general patterns.\n",
        "\n",
        "Dropout:\n",
        "\n",
        "* It's a regularization technique used to prevent overfitting in neural networks.\n",
        "* During training, randomly selected neurons (along with their connections) are temporarily ignored or \"dropped out.\"\n",
        "* It's like temporarily removing certain parts of your brain while studying to ensure that you don't rely too much on specific parts and learn more robustly.\n",
        "* Dropout helps in improving the generalization ability of the model by forcing it to learn redundant representations of the data, making it more robust to noise and variations."
      ],
      "metadata": {
        "id": "H9VDTLKlurN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Orange' size=5> Please play around with those values! </font>"
      ],
      "metadata": {
        "id": "9Rqo-xJT0mfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = 'categorical_crossentropy' # 'categorical_crossentropy' 'CategoricalHinge' 'mean_squared_error' or 'mean_absolute_error'\n",
        "batchsize = 256 # e.g. 32, 64, 128 or 256\n",
        "num_epoch = 5 # e.g. 5, 10, 20, 30 or 50\n",
        "dropout = 0.9 # e.g. 0.1, 0.2 or 0.9"
      ],
      "metadata": {
        "id": "Pw8KktVAmaEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is running the training and testing of the neural network. Depending on the chosen parameters and the model architecture this can take seconds to minutes to complete.\n",
        "\n",
        "<font color='Orange' size=5>\n",
        "The model architecture can be changed if you want to! You can add or remove layers.\n",
        "</font>"
      ],
      "metadata": {
        "id": "Zq_TNrB0yRuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),# First Layer of network\n",
        "    Dropout(dropout),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu'),# Second Layer of network\n",
        "    Dropout(dropout),\n",
        "    Dense(36, activation='relu'),# Third Layer of network\n",
        "    Dropout(dropout),\n",
        "    Dense(16, activation='relu'),# Fouth Layer of network\n",
        "    Dropout(dropout),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "time_callback = TimePerEpochCallback()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "wHGGwKdSfKqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the model architecture\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "Tbe8VSjmu94Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    epochs=num_epoch,\n",
        "                    batch_size=batchsize,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1,\n",
        "                    callbacks=[time_callback]\n",
        "                    )\n",
        "training_histories.append([history,loss_function,batchsize,dropout,num_epoch])\n",
        "training_features_histories.append(trainevents)\n",
        "print(\" \")\n",
        "# Model evaluation\n",
        "# Predictions on the test set\n",
        "y_pred_proba = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true_classes, y_pred_classes))\n",
        "print('Accuracy:', accuracy_score(y_true_classes, y_pred_classes))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "#print(\"Confusion Matrix:\")\n",
        "#print(conf_matrix)"
      ],
      "metadata": {
        "id": "JfBQ-jROvBin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <font color='Orange' size=5> Plot and compare your training with each other</font>\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
        "for i, hist in enumerate(training_histories):\n",
        "  # Plot training history\n",
        "  ax1.plot(hist[0].history['accuracy'], label=str(training_features_histories[i])+'\\n'+'loss_function '+ hist[1] +'\\n batchsize ' + str(hist[2]) + '\\n dropout ' + str(hist[3]) + '\\n epoch number ' + str(hist[4]))\n",
        "  #plt.plot(hist.history['val_accuracy'], label='val_accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend(prop={'size': 7},bbox_to_anchor=(0., -0.05, 1., 0.),)\n",
        "\n",
        "\n",
        "for i, hist in enumerate(training_histories):\n",
        "  ax2.plot(hist[0].history['time_per_epoch'])\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Time in seconds')\n",
        "#ax2.set_yscale('log')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5gHP5uNPoSUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion matrix\n",
        "A confusion matrix is a table used in classification tasks to evaluate how well a model is performing in terms of predicting classes. It helps us understand where the model is getting confused or making errors.\n",
        "\n",
        "Imagine you have a classification task, where you're trying to predict whether a cosmic ray event is proton, helium, oxygen or iron.\n",
        "\n",
        "Interpretation:\n",
        "* You want the majority of your predictions to fall on the diagonal (True Positive and True Negative) because those are correct predictions.\n",
        "* A large number of False Positives and False Negatives indicates areas where the model is making mistakes and may need improvement.\n",
        "* You can calculate more metrics like accuracy, precision, recall, and F1-score using the values from the confusion matrix to evaluate the model's performance further."
      ],
      "metadata": {
        "id": "hIf2-GmX8Brr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <font color='Orange' size=5> Plot the confusion matrix for your last training </font>\n",
        "classes = ['Proton','Helium','Oxygen','Iron']\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=classes, yticklabels=classes)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K5xijxNljyiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Receiver Operating Characteristic (ROC) Curves"
      ],
      "metadata": {
        "id": "kEzfyQlnxrVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ROC curves\n",
        "# Compute ROC-AUC for each class\n",
        "n_classes = y_categorical.shape[1]\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = plt.cm.get_cmap('tab10', n_classes)\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors(i), lw=2,\n",
        "             label=f'Class {label_encoder.inverse_transform([i])[0]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EBRqVzeJdcRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
        "print('ROC-AUC:', roc_auc)"
      ],
      "metadata": {
        "id": "v_Uas7vQbjaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='Orange' size=4> These results show that the classification network is working to determine what kind of primary particle was the cosmic ray but it's by now means perfect! Classification, despite its utility, is inherently imperfect due to the complexity and nuances of real-world data sets. </font>"
      ],
      "metadata": {
        "id": "oFBVaexQ1iWY"
      }
    }
  ]
}