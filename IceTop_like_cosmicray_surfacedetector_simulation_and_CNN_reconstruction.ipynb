{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eYlso3849y-O"
      ],
      "authorship_tag": "ABX9TyNE+BfyoIi4cATFbAao5cTI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthiasplum/CosmicRayML-Masterclass/blob/main/IceTop_like_cosmicray_surfacedetector_simulation_and_CNN_reconstruction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Air Shower Reconstruction with IceTop Detector Using Convolutional Neural Networks\n",
        "High-energy cosmic rays, originating from astrophysical sources, interact with Earth's atmosphere to create extensive air showers. These air showers are cascades of secondary particles that propagate through the atmosphere and can be detected by ground-based observatories such as the IceTop detector at the South Pole. Accurate reconstruction of air shower properties—such as the energy, impact point, and arrival direction—is essential for understanding the origin and nature of these cosmic rays.\n",
        "\n",
        "The IceTop detector, part of the IceCube Neutrino Observatory, consists of an array of ice-Cherenkov tanks spread over a square kilometer on the surface of the Antarctic ice. Each tank records signals generated by the Cherenkov light emitted when secondary particles from air showers pass through the ice. The timing and intensity of these signals provide crucial information about the characteristics of the original cosmic ray.\n",
        "\n",
        "Traditional methods for reconstructing air shower properties often rely on analytical and empirical techniques, which can be computationally intensive and limited by their assumptions and approximations. In recent years, machine learning, particularly deep learning, has emerged as a powerful tool for pattern recognition and regression tasks, offering the potential to enhance the precision and efficiency of air shower reconstructions.\n",
        "\n",
        "In this study, we propose using a Convolutional Neural Network (CNN) to reconstruct key parameters of air showers detected by IceTop. CNNs are well-suited for this task due to their ability to capture spatial hierarchies and patterns in data, making them effective for processing the grid-like layout of IceTop's detector array. By training the CNN on synthetic data that simulates the signals and timings recorded by the IceTop stations, we aim to predict the energy, impact point, and arrival direction of the primary cosmic rays.\n",
        "\n",
        "The rest of this document details the generation of synthetic data for IceTop stations, the design and implementation of the CNN model, the normalization of input and output data to improve model performance, and the evaluation of the model's predictions against true values. Our results demonstrate the potential of deep learning techniques to advance the field of cosmic ray physics and provide new insights into the most energetic particles in the universe.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wOnW8nOpx8zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import python packages"
      ],
      "metadata": {
        "id": "7FAWpkoy_6-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate"
      ],
      "metadata": {
        "id": "uveYaJGSt_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set detector constants and define sample parameters"
      ],
      "metadata": {
        "id": "WU_K8CiRADuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "NUM_STATIONS      = 81                                # Example number of stations (9x9 grid)\n",
        "GRID_SIZE         = 1000                              # Grid size in meters (1000m x 1000m)\n",
        "STATION_SPACING   = GRID_SIZE / np.sqrt(NUM_STATIONS)\n",
        "NUM_SAMPLES       = 1_000                             # Number of synthetic events\n",
        "SPEED_OF_LIGHT    = 3e8                               # Speed of the shower front in m/s (approx. speed of light)\n",
        "CURVATURE_A_SCALE = 4.823e-4                          # a for the curvature effect in ns/m^2\n",
        "CURVATURE_B_SCALE = 19.41                             # b for the curvature effect in ns\n",
        "CURVATURE_SIGMA   = 118.1                             # sigma for the curvature Gaussian in m\n",
        "\n",
        "R_REF = 125.  # Reference distance\n",
        "BETA  = 2.5   # Beta\n",
        "KAPPA = 0.303 # Kappa\n",
        "\n",
        "#Set trigger level of IceTop tanks\n",
        "TRIGGER_LEVEL = 0.125 # in log(VEM)\n",
        "\n",
        "USE_TRIGGER_LEVEL = False # boolean if trigger condition should be applied"
      ],
      "metadata": {
        "id": "FmJsoQk4uBhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define station position and python function"
      ],
      "metadata": {
        "id": "A49OcdrQASp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate station positions in a grid\n",
        "stations_x, stations_y = np.meshgrid(np.linspace(0, GRID_SIZE, int(np.sqrt(NUM_STATIONS))),\n",
        "                                     np.linspace(0, GRID_SIZE, int(np.sqrt(NUM_STATIONS))))\n",
        "stations_positions = np.vstack([stations_x.ravel(), stations_y.ravel()]).T\n",
        "\n",
        "def logS(perpendicular_distance,log_energy):\n",
        "    log_s_ref = 1. * log_energy - 6.\n",
        "\n",
        "    if perpendicular_distance!=0:\n",
        "      log_s = log_s_ref *  (perpendicular_distance/R_REF)**(-BETA - np.log10(perpendicular_distance/R_REF))\n",
        "    else:\n",
        "      log_s = np.nan\n",
        "\n",
        "    return log_s  # in log(VEM)\n",
        "\n",
        "# Function to simulate an air shower hit on the IceTop stations\n",
        "def simulate_air_shower(log_energy,impact_time,  impact_x, impact_y, azimuth, zenith):\n",
        "    signals = []\n",
        "    times = []\n",
        "    triggered = []\n",
        "\n",
        "    # Calculate the direction of the shower front in cartesian coordinates\n",
        "    direction_cosines = np.array([np.sin(zenith) * np.cos(azimuth),\n",
        "                                  np.sin(zenith) * np.sin(azimuth),\n",
        "                                  np.cos(zenith)])\n",
        "\n",
        "    # Calculate the distance from the stations to the impact point\n",
        "    for pos in stations_positions:\n",
        "        dist_x = pos[0] - impact_x\n",
        "        dist_y = pos[1] - impact_y\n",
        "\n",
        "        distance_vector = np.array([dist_x, dist_y, 0.])  # Distance vector from the impact point to the station\n",
        "\n",
        "        dist = np.linalg.norm(distance_vector)  # Calculate the distance from the stations to the impact point\n",
        "\n",
        "        # Calculate the perpendicular distance from the shower axis\n",
        "        projection_length = np.dot(distance_vector, direction_cosines)\n",
        "        projection_vector = projection_length * direction_cosines\n",
        "        perpendicular_vector = distance_vector - projection_vector\n",
        "\n",
        "        perpendicular_distance = np.linalg.norm(perpendicular_vector)\n",
        "\n",
        "        # Calculate the signal strength in each station\n",
        "        signal = logS(perpendicular_distance,log_energy)\n",
        "\n",
        "        # Time delay based on the projection of the distance vector onto the direction cosines\n",
        "        plane_wave_delay = np.dot(distance_vector,direction_cosines) / SPEED_OF_LIGHT * 1e9 # in ns\n",
        "\n",
        "        # Curvature delay\n",
        "        curvature_delay = (CURVATURE_A_SCALE * perpendicular_distance**2 + CURVATURE_B_SCALE * (1 - np.exp(-perpendicular_distance**2 / (2*CURVATURE_SIGMA**2))))\n",
        "\n",
        "        total_time_delay = impact_time + plane_wave_delay + curvature_delay\n",
        "\n",
        "        # Add the signal and time delay to the lists\n",
        "        if USE_TRIGGER_LEVEL:\n",
        "          if signal > TRIGGER_LEVEL:\n",
        "            triggered.append(True)\n",
        "            signals.append(signal)\n",
        "            times.append(total_time_delay)\n",
        "          else:\n",
        "            triggered.append(False)\n",
        "            signals.append(0)\n",
        "            times.append(-100)\n",
        "        else:\n",
        "          signals.append(signal)\n",
        "          times.append(total_time_delay)\n",
        "\n",
        "    return np.array(signals), np.array(times), np.array(triggered)"
      ],
      "metadata": {
        "id": "ehHAKVLPuHCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generation of synthetic cosmic ray data"
      ],
      "metadata": {
        "id": "L71tp2wdAJQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic air shower data\n",
        "X_signals   = []\n",
        "X_times     = []\n",
        "X_triggered = []\n",
        "y_logS125   = []\n",
        "y_energy    = []\n",
        "y_impact    = []\n",
        "y_direction = []\n",
        "\n",
        "for _ in range(NUM_SAMPLES):\n",
        "    log_energy  = np.random.uniform(6, 9)                                           # Energy in Log(E/GeV)\n",
        "    impact_x    = np.random.uniform(0, GRID_SIZE)                                   # Shower core position x in m\n",
        "    impact_y    = np.random.uniform(0, GRID_SIZE)                                   # Shower core position y in m\n",
        "    azimuth     = np.random.uniform(np.deg2rad(0), np.deg2rad(360))                 # Shower direction azimuth in degree\n",
        "    coszenith   = np.random.uniform(np.cos(np.deg2rad(0)), np.cos(np.deg2rad(60)))  # Shower direction cos(zenith) in degree\n",
        "\n",
        "    signals, times, triggered  = simulate_air_shower(log_energy, 5000, impact_x, impact_y, azimuth, np.arccos(coszenith))\n",
        "\n",
        "    X_signals.append(signals)\n",
        "    X_times.append(times)\n",
        "    X_triggered.append(triggered)\n",
        "    y_logS125.append(logS(125,log_energy))\n",
        "    y_energy.append(log_energy)\n",
        "    y_impact.append([impact_x, impact_y])\n",
        "    y_direction.append([coszenith,np.sin(azimuth),np.cos(azimuth)])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_signals = np.array(X_signals)\n",
        "X_times = np.array(X_times)\n",
        "y_energy = np.array(y_energy)\n",
        "y_impact = np.array(y_impact)\n",
        "y_direction = np.array(y_direction)"
      ],
      "metadata": {
        "id": "ufMWox-HuijY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot some example showers (Can be skipped)"
      ],
      "metadata": {
        "id": "eYlso3849y-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for example_shower in range(NUM_SAMPLES)[:10]:\n",
        "\n",
        "  # Calculate the direction vector\n",
        "  direction_vector = np.array([\n",
        "      np.sin(np.arccos(y_direction[example_shower][0])) * y_direction[example_shower][2],\n",
        "      np.sin(np.arccos(y_direction[example_shower][0])) * y_direction[example_shower][1],\n",
        "      y_direction[example_shower][0]\n",
        "  ])\n",
        "\n",
        "  # For 2D projection, we use only the x and y components\n",
        "  x_component = direction_vector[0]\n",
        "  y_component = direction_vector[1]\n",
        "\n",
        "  # Visualize examples\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "  ax[0].text(0.1, 1.15,\\\n",
        "             'log(E/GeV): '+str(round(y_energy[example_shower],2)),\\\n",
        "             ha='center', va='center', transform=ax[0].transAxes)\n",
        "  ax[0].text(0.75, 1.15,\\\n",
        "             'Direction\\n Zenith: ' +str(round(np.arccos(y_direction[example_shower][0])*180./np.pi,2))+' deg \\n Azimuth: ' +str(round(np.arctan2(y_direction[example_shower][1],y_direction[example_shower][2])*180./np.pi,2))+' deg',\\\n",
        "             ha='center', va='center', transform=ax[0].transAxes)\n",
        "  sc1 = ax[0].scatter(stations_positions[:, 0],\\\n",
        "                      stations_positions[:, 1],\\\n",
        "                      c=X_signals[example_shower],\\\n",
        "                      s=10*X_signals[example_shower],\\\n",
        "                      cmap='rainbow')\n",
        "  fig.colorbar(sc1,ax=ax[0],label='Signal Strength in \"log(VEM)\"')\n",
        "  ax[0].set_aspect('equal')\n",
        "  ax[0].set_xlabel('x in m')\n",
        "  ax[0].set_ylabel('y in m')\n",
        "  ax[0].set_xlim(-10,GRID_SIZE+10)\n",
        "  ax[0].set_ylim(-10,GRID_SIZE+10)\n",
        "  ax[0].set_title('Station Signal Strengths')\n",
        "  ax[0].plot(y_impact[example_shower][0],y_impact[example_shower][1], 'rx')\n",
        "  ax[0].quiver(y_impact[example_shower][0],y_impact[example_shower][1],\\\n",
        "               100 * x_component,100 * y_component, angles='xy', scale_units='xy',\\\n",
        "               scale=1, color='r', label='Shower Axis')\n",
        "\n",
        "\n",
        "  sc2 = ax[1].scatter(np.ma.masked_less(stations_positions[:, 0],0),\\\n",
        "                      np.ma.masked_less(stations_positions[:, 1],0),\\\n",
        "                      c=np.ma.masked_less(X_times[example_shower],0),\\\n",
        "                      cmap='rainbow')\n",
        "  fig.colorbar(sc2,ax=ax[1],label='Hit Times in ns')\n",
        "  ax[1].set_aspect('equal')\n",
        "  ax[1].set_xlabel('x in m')\n",
        "  ax[1].set_ylabel('y in m')\n",
        "  ax[1].set_xlim(-10,GRID_SIZE+10)\n",
        "  ax[1].set_ylim(-10,GRID_SIZE+10)\n",
        "  ax[1].set_title('Station Hit Times')\n",
        "  ax[1].plot(y_impact[example_shower][0],y_impact[example_shower][1], 'rx')\n",
        "  ax[1].quiver(y_impact[example_shower][0],y_impact[example_shower][1],\\\n",
        "               100 * x_component,100 * y_component, angles='xy', scale_units='xy',\\\n",
        "               scale=1, color='r', label='Shower Axis')\n",
        "\n",
        "  #for i, txt in enumerate(X_times[example_shower]):\n",
        "      #ax.annotate(txt, (x[i], y[i]))\n",
        "      #ax[1].annotate(round(txt,1),(stations_positions[:, 0][i]+10, stations_positions[:, 1][i]+10))\n",
        "  plt.savefig(f'example_{example_shower}.png')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RvQB_eTNufNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reshaping, splitting and scaling the input and the output variables"
      ],
      "metadata": {
        "id": "jM_533wt9_Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_signals and X_times are your features and y_energy, y_impact, y_direction are your labels\n",
        "# Split the data into training and testing sets\n",
        "X_signals_train, X_signals_test, X_times_train, X_times_test,\\\n",
        "y_energy_train, y_energy_test, y_impact_train, y_impact_test,\\\n",
        "y_direction_train, y_direction_test = train_test_split(X_signals, X_times, y_energy, y_impact, y_direction, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data to fit into a CNN: (num_samples, height, width, channels)\n",
        "# Here, height and width are the dimensions of the grid (sqrt(NUM_STATIONS) x sqrt(NUM_STATIONS))\n",
        "height, width = int(np.sqrt(NUM_STATIONS)), int(np.sqrt(NUM_STATIONS))\n",
        "\n",
        "# Normalize the input features\n",
        "scaler_signals  = StandardScaler()\n",
        "scaler_times    = StandardScaler()\n",
        "\n",
        "X_signals_train = scaler_signals.fit_transform(X_signals_train.reshape(-1, NUM_STATIONS)).reshape(-1, height, width, 1)\n",
        "X_signals_test  = scaler_signals.transform(X_signals_test.reshape(-1, NUM_STATIONS)).reshape(-1, height, width, 1)\n",
        "X_times_train   = scaler_times.fit_transform(X_times_train.reshape(-1, NUM_STATIONS)).reshape(-1, height, width, 1)\n",
        "X_times_test    = scaler_times.transform(X_times_test.reshape(-1, NUM_STATIONS)).reshape(-1, height, width, 1)\n",
        "\n",
        "# Stack the signals and times to create a two-channel input\n",
        "X_train = np.concatenate([X_signals_train, X_times_train], axis=-1)\n",
        "X_test  = np.concatenate([X_signals_test, X_times_test], axis=-1)\n",
        "\n",
        "# Normalize the output targets\n",
        "scaler_energy     = StandardScaler()\n",
        "scaler_impact     = StandardScaler()\n",
        "scaler_direction  = StandardScaler()\n",
        "\n",
        "y_energy_train = scaler_energy.fit_transform(y_energy_train.reshape(-1, 1))\n",
        "y_energy_test = scaler_energy.transform(y_energy_test.reshape(-1, 1))\n",
        "y_impact_train = scaler_impact.fit_transform(y_impact_train)\n",
        "y_impact_test = scaler_impact.transform(y_impact_test)\n",
        "y_direction_train = scaler_direction.fit_transform(y_direction_train)\n",
        "y_direction_test = scaler_direction.transform(y_direction_test)"
      ],
      "metadata": {
        "id": "Vfm-WRxAXrwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the CNN model"
      ],
      "metadata": {
        "id": "eDKTCfYc93lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input\n",
        "input_layer = Input(shape=(height, width, 2))\n",
        "\n",
        "# Convolutional layers\n",
        "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layer)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2), padding='same')(conv1)\n",
        "\n",
        "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(pool1)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2), padding='same')(conv2)\n",
        "\n",
        "conv3 = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(pool2)\n",
        "pool3 = MaxPooling2D(pool_size=(2, 2), padding='same')(conv3)\n",
        "\n",
        "# Flatten the output\n",
        "flatten = Flatten()(pool3)\n",
        "\n",
        "# Fully connected layers for energy, impact point, and arrival direction\n",
        "dense1 = Dense(256, activation='relu')(flatten)\n",
        "energy_output = Dense(1, name='energy_output')(dense1)\n",
        "\n",
        "dense2 = Dense(256, activation='relu')(flatten)\n",
        "impact_output = Dense(2, name='impact_output')(dense2)\n",
        "\n",
        "dense3 = Dense(256, activation='relu')(flatten)\n",
        "direction_output = Dense(3, name='direction_output')(dense3)\n",
        "\n",
        "# Combine into a single model\n",
        "model = Model(inputs=input_layer, outputs=[energy_output, impact_output, direction_output])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'energy_output': 'mse', 'impact_output': 'mse', 'direction_output': 'mse'},\n",
        "              metrics={'energy_output': 'mae', 'impact_output': 'mae', 'direction_output': 'mae'})\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "dvI_V7mkYZCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the model architecture\n",
        "#plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "C-dMus6uuIqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train,\n",
        "                    {'energy_output': y_energy_train, 'impact_output': y_impact_train, 'direction_output': y_direction_train},\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "# Save the model\n",
        "#model.save('icetop_cnn_model.keras')"
      ],
      "metadata": {
        "id": "7_umObgQZdRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, energy_loss, impact_loss, direction_loss, energy_mae, impact_mae, direction_mae = model.evaluate(X_test,\n",
        "                {'energy_output': y_energy_test, 'impact_output': y_impact_test, 'direction_output': y_direction_test})\n",
        "\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Energy MAE: {energy_mae}\")\n",
        "print(f\"Impact MAE: {impact_mae}\")\n",
        "print(f\"Direction MAE: {direction_mae}\")"
      ],
      "metadata": {
        "id": "nX19OZWkaL-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['energy_output_loss'])\n",
        "plt.plot(history.history['val_energy_output_loss'])\n",
        "plt.title('Energy Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['impact_output_loss'])\n",
        "plt.plot(history.history['val_impact_output_loss'])\n",
        "plt.title('Impact Point Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['direction_output_loss'])\n",
        "plt.plot(history.history['val_direction_output_loss'])\n",
        "plt.title('Direction Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C6ZWR5yFaXip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Separate the predictions\n",
        "y_energy_pred = scaler_energy.inverse_transform(y_pred[0])\n",
        "y_impact_pred = scaler_impact.inverse_transform(y_pred[1])\n",
        "y_direction_pred = scaler_direction.inverse_transform(y_pred[2])\n",
        "\n",
        "# Inverse transform the true values\n",
        "y_energy_test_inv = scaler_energy.inverse_transform(y_energy_test)\n",
        "y_impact_test_inv = scaler_impact.inverse_transform(y_impact_test)\n",
        "y_direction_test_inv = scaler_direction.inverse_transform(y_direction_test)"
      ],
      "metadata": {
        "id": "xGmHIbitdfMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Energy Predictions vs True Values\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.scatter(y_energy_test_inv, y_energy_pred, alpha=0.5)\n",
        "plt.plot([min(y_energy_test_inv),max(y_energy_test_inv)],\\\n",
        "         [min(y_energy_test_inv),max(y_energy_test_inv)],\\\n",
        "         'r--')\n",
        "plt.xlabel('True Energy')\n",
        "plt.ylabel('Predicted Energy')\n",
        "plt.title('Energy: True vs Predicted')\n",
        "\n",
        "# Plot Impact Point Predictions vs True Values\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.scatter(y_impact_test_inv[:, 0], y_impact_pred[:, 0], alpha=0.5, label='X-coordinate')\n",
        "plt.scatter(y_impact_test_inv[:, 1], y_impact_pred[:, 1], alpha=0.5, label='Y-coordinate')\n",
        "plt.plot([min(y_impact_test_inv[:, 1]),max(y_impact_test_inv[:, 1])],\\\n",
        "         [min(y_impact_test_inv[:, 1]),max(y_impact_test_inv[:, 1])],\\\n",
        "         'r--')\n",
        "plt.xlabel('True Impact Point')\n",
        "plt.ylabel('Predicted Impact Point')\n",
        "plt.title('Impact Point: True vs Predicted')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Direction Predictions vs True Values\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.scatter(y_direction_test_inv[:, 0], y_direction_pred[:, 0], alpha=0.5, label='cos(Zenith)')\n",
        "\n",
        "plt.plot([min(y_direction_test_inv[:, 0]),max(y_direction_test_inv[:, 0])],\\\n",
        "         [min(y_direction_test_inv[:, 0]),max(y_direction_test_inv[:, 0])], \\\n",
        "         'r--')\n",
        "plt.xlabel('True Direction')\n",
        "plt.ylabel('Predicted Direction')\n",
        "plt.title('Direction: True vs Predicted')\n",
        "# Plot Direction Predictions vs True Values\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.scatter(np.arctan2(y_direction_test_inv[:, 1],y_direction_test_inv[:, 2]),\\\n",
        "            np.arctan2(y_direction_pred[:, 1],y_direction_pred[:, 2]),\\\n",
        "            alpha=0.5, label='Azimuth')\n",
        "#plt.scatter(y_direction_test[:, 2], y_direction_pred[:, 2], alpha=0.5, label='cos(Azimuth)')\n",
        "plt.plot([min(np.arctan2(y_direction_test_inv[:, 1],y_direction_test_inv[:, 2])),max(np.arctan2(y_direction_test[:, 1],y_direction_test[:, 2]))],\\\n",
        "         [min(np.arctan2(y_direction_test_inv[:, 1],y_direction_test_inv[:, 2])),max(np.arctan2(y_direction_test[:, 1],y_direction_test[:, 2]))],\\\n",
        "         'r--')\n",
        "\n",
        "plt.xlabel('True Direction')\n",
        "plt.ylabel('Predicted Direction')\n",
        "plt.title('Direction: True vs Predicted')\n",
        "#plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lGFxsj9td2Jh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}